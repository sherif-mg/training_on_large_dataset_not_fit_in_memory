{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset from kaggle\n"
      ],
      "metadata": {
        "id": "l9KDhdezuP2u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DlQJwvzt6EK",
        "outputId": "44969311-b508-4eda-c7e9-231b4995908b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2022.6.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (6.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/l33tc0d3r/indian-food-classification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nmgNqmiuHQM",
        "outputId": "937712be-c98e-4ef7-930b-e5afd248dbca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: sherifmagdyabdellah\n",
            "Your Kaggle Key: ··········\n",
            "Downloading indian-food-classification.zip to ./indian-food-classification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.48G/1.48G [00:10<00:00, 156MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "store all images in one folder"
      ],
      "metadata": {
        "id": "U8x7JPimwhlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "!mkdir all_images\n",
        "\n",
        "train_dir = '/content/indian-food-classification'\n",
        "dest_dir = '/content/all_images'\n",
        "counter = 0\n",
        "labels=[]\n",
        "files_path=[]\n",
        "for subdir, dirs, files in os.walk(train_dir):\n",
        "    for file in files:\n",
        "      if(file.split(\".\")[-1] in [\"png\",\"jpg\",\"jpeg\"]):\n",
        "          full_path = os.path.join(subdir, file)\n",
        "          full_path_dest = os.path.join(dest_dir,f\"={subdir.split('/')[-1]}={file}\")\n",
        "          img = cv2.imread(full_path)\n",
        "          img=cv2.resize(img,(240,240))\n",
        "          cv2.imwrite(full_path_dest, img)\n",
        "          labels.append(subdir.split('/')[-1])\n",
        "          files_path.append(full_path_dest)\n",
        "          counter = counter + 1\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dydbrpYu2Gk",
        "outputId": "12c65f67-d0bf-4a84-98f5-f02916acfcad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le=LabelEncoder()\n",
        "le.fit(labels)\n",
        "labels_en = le.transform(labels)"
      ],
      "metadata": {
        "id": "mgugc33m3RT_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ours labels encode{np.unique(labels_en)}\")\n",
        "print(\"we have 20 differnet class\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JrKxFHG3yjU",
        "outputId": "6a720c36-aef1-46e0-dafe-2bccf8d55304"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ours labels encode[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
            "we have 20 differnet class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save our data pathes and labels"
      ],
      "metadata": {
        "id": "5X50G6Dc4X5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('files_path.npy', files_path)\n",
        "np.save('labels_en.npy', labels_en)"
      ],
      "metadata": {
        "id": "tlCFqsQY4L2J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "files_path_shuffle,labels_en_shuffle=shuffle(files_path, labels_en)"
      ],
      "metadata": {
        "id": "BAtzMel-4g6Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('files_path_shuffle.npy', files_path_shuffle)\n",
        "np.save('labels_en_shuffle.npy', labels_en_shuffle)"
      ],
      "metadata": {
        "id": "O7iwWgkB40Y4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create  our Custom Generator"
      ],
      "metadata": {
        "id": "XNwW1xbJ_4qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "  def __init__(self, image_filenames, labels, batch_size) :\n",
        "    self.image_filenames = image_filenames\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    \n",
        "  def __len__(self) :\n",
        "    return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int32)\n",
        "  \n",
        "  \n",
        "  def __getitem__(self, idx) :\n",
        "    batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    \n",
        "    return (np.array([cv2.imread(str(file_name))for file_name in batch_x])/255\n",
        "            , np.array(batch_y))"
      ],
      "metadata": {
        "id": "u7iNp6hF5OV0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(files_path_shuffle, labels_en_shuffle, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "d8DLk9Ru_GyY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_filenames=np.array(X_train_filenames)\n",
        "X_val_filenames=np.array(X_val_filenames)\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "metadata": {
        "id": "v1R_tcnL_jTa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_filenames.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(X_val_filenames.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "np.save('X_train_filenames.npy', X_train_filenames)\n",
        "np.save('y_train.npy', y_train)\n",
        "\n",
        "np.save('X_val_filenames.npy', X_val_filenames)\n",
        "np.save('y_val.npy', y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRsBYZ62_aPN",
        "outputId": "0632a69e-753f-4a2e-f6ec-b694951a2d46"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5004,)\n",
            "(5004,)\n",
            "(1252,)\n",
            "(1252,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create our model"
      ],
      "metadata": {
        "id": "HbRHJtjq_zvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "VGG16=tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    input_shape=(240,240,3))\n",
        "\n",
        "for layer in VGG16.layers[:]:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "fC1YJva8_y5p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(VGG16)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(20, activation='softmax'))\n",
        "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvckCBmUH71H",
        "outputId": "c0ac36c8-895a-41a3-cf77-29690ef7119b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               2508900   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                2020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,225,608\n",
            "Trainable params: 2,510,920\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "my_training_batch_generator = My_Custom_Generator(X_train_filenames, y_train, batch_size)\n",
        "my_validation_batch_generator = My_Custom_Generator(X_val_filenames, y_val, batch_size)"
      ],
      "metadata": {
        "id": "le-gtaf7ItzY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator=my_training_batch_generator,\n",
        "                   steps_per_epoch =len(X_train_filenames) // batch_size,\n",
        "                   epochs = 5,\n",
        "                   verbose = 1,\n",
        "                   validation_data = my_validation_batch_generator,\n",
        "                   validation_steps = len(X_val_filenames) // batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kfnRvy1IVAK",
        "outputId": "ceb7210d-60d7-4ab4-eaf1-8f266dc75ddf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78/78 [==============================] - 46s 516ms/step - loss: 2.5144 - accuracy: 0.2767 - val_loss: 1.7481 - val_accuracy: 0.5066\n",
            "Epoch 2/5\n",
            "78/78 [==============================] - 38s 484ms/step - loss: 1.4776 - accuracy: 0.5593 - val_loss: 1.4076 - val_accuracy: 0.6003\n",
            "Epoch 3/5\n",
            "78/78 [==============================] - 38s 481ms/step - loss: 1.0709 - accuracy: 0.6806 - val_loss: 1.2701 - val_accuracy: 0.6332\n",
            "Epoch 4/5\n",
            "78/78 [==============================] - 37s 479ms/step - loss: 0.8197 - accuracy: 0.7609 - val_loss: 1.1981 - val_accuracy: 0.6562\n",
            "Epoch 5/5\n",
            "78/78 [==============================] - 38s 482ms/step - loss: 0.6471 - accuracy: 0.8219 - val_loss: 1.1814 - val_accuracy: 0.6513\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f76968fc110>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i don't care about model or to score highest accuracy but our goal to train model with large dataset which doesn't fit in memory"
      ],
      "metadata": {
        "id": "8xmD7HWSMzTM"
      }
    }
  ]
}